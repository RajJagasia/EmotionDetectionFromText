{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47154ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk imports\n",
    "from nltk.tokenize import word_tokenize  # tokenize the text == the text is splitted into words in list\n",
    "from nltk.corpus import stopwords  # this contain common stop words that has no effect in analysis\n",
    "from nltk.stem import WordNetLemmatizer  # Lemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer  # bags of words and TF IDF\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, confusion_matrix, make_scorer  # classification Metrics\n",
    "from sklearn.naive_bayes import MultinomialNB  # Multiclassification\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import StratifiedKFold  # For stratified splitting (helpful in imbalanced data)\n",
    "from sklearn.preprocessing import LabelBinarizer  # for Categorical features\n",
    "from sklearn.model_selection import GridSearchCV  # for tuning parameters\n",
    "from sklearn.model_selection import train_test_split  # splitting dataset\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn import pipeline\n",
    "from sklearn import linear_model\n",
    "\n",
    "# gensim imports\n",
    "from gensim.models import KeyedVectors  # to save and load vectors\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import catboost as cbt\n",
    "\n",
    "# tensorflow and keras\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping , ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "\n",
    "# Hugging Face Transformers\n",
    "from transformers import (pipeline , BertTokenizer,\n",
    "                          TFBertForSequenceClassification,\n",
    "                          InputExample, InputFeatures , \n",
    "                         AutoTokenizer, TFAutoModelForSequenceClassification,\n",
    "                         TFRobertaModel, TFGPT2Model, RobertaTokenizer, GPT2Tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8623b067",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/tweets-new-data/tweet_and_emotion.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f37b39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop('tweet_id',axis =1, inplace= True)\n",
    "df.columns = [ 'Sentence','Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e482cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a124df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Sentiment.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02ce36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping to the sentiment column \n",
    "\n",
    "dicto = {'anger': 0,'fear': 1, 'joy':2, 'sadness':3, 'neutral':4}\n",
    "df = df[df.Sentiment.isin(dicto.keys())]\n",
    "df.Sentiment = df.Sentiment.map(dicto)\n",
    "df.Sentence = df.Sentence.apply(lambda x: re.sub(r'(@\\S+)|(http\\S+)|(www\\.\\S+)', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5598af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading pretrained google news word2vec embedding 300D\n",
    "word2vec_pretrained = KeyedVectors.load_word2vec_format(\"../input/googles-trained-word2vec-model-in-python/GoogleNews-vectors-negative300.bin\",binary=True)\n",
    "word2vec_pretrained_dict = dict(zip(word2vec_pretrained.key_to_index.keys(),\n",
    "                                    word2vec_pretrained.vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514fb198",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(word2vec_pretrained_dict.values())[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b54be57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['Sentence'] = df['Sentence'].apply(process_text)\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(df.Sentence, df.Sentiment, test_size = 0.2,\n",
    "                                                 random_state = 42, stratify= df.Sentiment, shuffle = True)\n",
    "y_train_enc = np_utils.to_categorical(y_train, 5)\n",
    "y_test_enc = np_utils.to_categorical(y_test, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ed1a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = tf.keras.preprocessing.text.Tokenizer(num_words=None)\n",
    "\n",
    "token.fit_on_texts(list(X_train) + list(X_test)) # fits tokens on texts\n",
    "xtrain_seq = token.texts_to_sequences(X_train) # text to sequences converts the sentence words to number sequences\n",
    "xtest_seq = token.texts_to_sequences(X_test)\n",
    "\n",
    "#zero pad sequences\n",
    "xtrain_pad = pad_sequences(xtrain_seq,padding='post') # zero padding all sentences to have the same shape as the largest one\n",
    "xtest_pad = pad_sequences(xtest_seq,padding='post')\n",
    "\n",
    "word_index = token.word_index # returns the word index that have been tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ab3ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create embedding matrix for words that we have in dataset\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index)+1, 300))\n",
    "for word,i in word_index.items():\n",
    "    embedding_vector = word2vec_pretrained_dict.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a598d5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Custom Metrics (F1-Score)\n",
    "\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m_():\n",
    "    def f1_m(y_true, y_pred):\n",
    "        precision = precision_m(y_true, y_pred)\n",
    "        recall = recall_m(y_true, y_pred)\n",
    "        return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "    return f1_m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc2bced",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bi = Sequential()\n",
    "model_bi.add(Embedding(len(word_index)+1, 300, weights=[embedding_matrix], trainable = False))\n",
    "\n",
    "model_bi.add(SpatialDropout1D(0.3))\n",
    "model_bi.add(Bidirectional(LSTM(300, dropout = 0.3, recurrent_dropout = 0.3)))\n",
    "\n",
    "model_bi.add(Dense(1024, activation = 'relu'))\n",
    "model_bi.add(Dropout(0.8))\n",
    "\n",
    "\n",
    "model_bi.add(Dense(1024, activation = 'relu'))\n",
    "model_bi.add(Dropout(0.8))\n",
    "\n",
    "model_bi.add(Dense(5))\n",
    "model_bi.add(Activation('softmax'))\n",
    "model_bi.summary()\n",
    "\n",
    "model_bi.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = f1_m_())\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 15, verbose = 1, mode = 'auto')\n",
    "history = model_bi.fit(xtrain_pad, y=y_train_enc, batch_size = 128, epochs = 100, verbose=1,\n",
    "                       validation_data = (xtest_pad, y_test_enc),callbacks = [earlystop])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa44bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_bi.predict(xtest_pad)\n",
    "print(classification_report(np.argmax(y_test_enc, axis=1), np.argmax(y_pred , axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f25e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
